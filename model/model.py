import os
import onnxruntime_genai as oga
from huggingface_hub import snapshot_download

class DeepSeekAMDModel:
    def __init__(self, model_dir="https://huggingface.co/vijaya-chandra/local_llm"):
        """
        Initializes the Aegis Workspace LLM.
        Automatically downloads the AMD-optimized DeepSeek model if missing.
        """
        self.model_dir = model_dir
        # The official AMD Ryzen AI 1.7 optimized repository for the DeepSeek Distill model
        self.repo_id = "amd/DeepSeek-R1-Distill-Qwen-1.5B-onnx-ryzenai-npu" 
        
        self._ensure_model_downloaded()
        
        print("Loading DeepSeek into AMD Ryzen AI Heterogeneous Subsystem...")
        # The OGA Model class automatically reads genai_config.json to route to NPU/iGPU
        self.model = oga.Model(self.model_dir)
        self.tokenizer = oga.Tokenizer(self.model)
        print("Model loaded and hardware orchestration ready!")

    def _ensure_model_downloaded(self):
        """Programmatically downloads the model so the user doesn't have to use CLI."""
        config_path = os.path.join(self.model_dir, "genai_config.json")
        if not os.path.exists(config_path):
            print(f"Model not found locally. Downloading {self.repo_id} from Hugging Face...")
            os.makedirs(self.model_dir, exist_ok=True)
            
            # Downloads the specific AMD ONNX files directly into your project folder
            snapshot_download(
                repo_id=self.repo_id,
                local_dir=self.model_dir,
                local_dir_use_symlinks=False,
                ignore_patterns=["*.md", "*.txt"] # Skips unnecessary text files
            )
            print("Download complete. Model is now air-gapped.")

    def chat(self, prompt, max_tokens=512):
        """Generates the response entirely locally using AMD hardware."""
        # Standard chat template format for DeepSeek/Qwen models
        formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        input_tokens = self.tokenizer.encode(formatted_prompt)
        
        params = oga.GeneratorParams(self.model)
        params.set_search_options(max_length=max_tokens, past_present_share_buffer=True)
        params.input_ids = input_tokens

        generator = oga.Generator(self.model, params)
        
        print("Aegis Workspace: ", end="", flush=True)
        response = ""
        
        # Stream the tokens as they are generated by the iGPU
        while not generator.is_done():
            generator.compute_logits()
            generator.generate_next_token()
            new_token = generator.get_next_tokens()
            text_chunk = self.tokenizer.decode([new_token])
            print(text_chunk, end="", flush=True)
            response += text_chunk
            
        print("\n") 
        return response

# You can test this file directly by running it:
if __name__ == "__main__":
    # Make sure you have installed: pip install onnxruntime-genai huggingface_hub
    ai_agent = DeepSeekAMDModel()
    
    test_question = "Why is local AI important for corporate data security?"
    print(f"User: {test_question}")
    ai_agent.chat(test_question)
